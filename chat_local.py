#!/usr/bin/env python3
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM
from peft import PeftModel    
import time
import os

def clear_screen():
    """Clear the terminal screen"""
    os.system('cls' if os.name == 'nt' else 'clear')

def print_banner():
    """Print a beautiful SAMNEH AI banner"""
    banner = r"""
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘                                                                               â•‘
â•‘   â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   â–ˆâ–ˆâ–ˆ    â–ˆâ–ˆâ–ˆ â–ˆâ–ˆâ–ˆ    â–ˆâ–ˆ â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ â–ˆâ–ˆ   â–ˆâ–ˆ      â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  â–ˆâ–ˆ       â•‘
â•‘   â–ˆâ–ˆ       â–ˆâ–ˆ   â–ˆâ–ˆ  â–ˆâ–ˆâ–ˆâ–ˆ  â–ˆâ–ˆâ–ˆâ–ˆ â–ˆâ–ˆ â–ˆâ–ˆ   â–ˆâ–ˆ â–ˆâ–ˆ      â–ˆâ–ˆ   â–ˆâ–ˆ     â–ˆâ–ˆ   â–ˆâ–ˆ â–ˆâ–ˆ       â•‘
â•‘   â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  â–ˆâ–ˆ â–ˆâ–ˆâ–ˆâ–ˆ â–ˆâ–ˆ â–ˆâ–ˆ  â–ˆâ–ˆ  â–ˆâ–ˆ â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ â–ˆâ–ˆ       â•‘
â•‘        â–ˆâ–ˆ  â–ˆâ–ˆ   â–ˆâ–ˆ  â–ˆâ–ˆ  â–ˆâ–ˆ  â–ˆâ–ˆ â–ˆâ–ˆ   â–ˆâ–ˆ â–ˆâ–ˆ â–ˆâ–ˆ      â–ˆâ–ˆ   â–ˆâ–ˆ     â–ˆâ–ˆ   â–ˆâ–ˆ â–ˆâ–ˆ       â•‘
â•‘   â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  â–ˆâ–ˆ   â–ˆâ–ˆ  â–ˆâ–ˆ      â–ˆâ–ˆ â–ˆâ–ˆ    â–ˆâ–ˆâ–ˆâ–ˆ â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ â–ˆâ–ˆ   â–ˆâ–ˆ     â–ˆâ–ˆ   â–ˆâ–ˆ â–ˆâ–ˆ       â•‘
â•‘                                                                               â•‘
â•‘                     ğŸ§  SAMNEH AI - PHILOSOPHICAL COMPANION ğŸ§                  â•‘
â•‘                          Powered by Advanced ML Training                     â•‘
â•‘                                                                               â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
"""
    print("\033[36m" + banner + "\033[0m")  # Cyan color

def print_loading_animation(text):
    """Print loading animation"""
    print(f"\033[33mâš¡ {text}\033[0m", end="")
    for i in range(3):
        time.sleep(0.5)
        print(".", end="", flush=True)
    print(" âœ…")

def load_socrates_model():
    """Load the fine-tuned Socrates model"""
    base_model = "mistralai/Mistral-7B-v0.1"
    
    print_loading_animation("Initializing Socrates AI")
    tokenizer = AutoTokenizer.from_pretrained(base_model)
    
    print_loading_animation("Loading neural pathways")
    model = AutoModelForCausalLM.from_pretrained(
        base_model,
        torch_dtype=torch.float16,
        low_cpu_mem_usage=True
    )
    
    print_loading_animation("Integrating philosophical wisdom")
    model = PeftModel.from_pretrained(model, "./socrates-dpo-adapters")
    
    # Move to GPU if available
    if torch.cuda.is_available():
        model = model.cuda()
        print("\033[32mğŸš€ GPU acceleration enabled\033[0m")
    else:
        print("\033[33mğŸ’» Running on CPU\033[0m")
    
    tokenizer.pad_token = tokenizer.eos_token
    tokenizer.padding_side = "left"
    
    print("\033[32mâœ¨ Socrates AI is ready for philosophical dialogue!\033[0m\n")
    return model, tokenizer

def print_separator():
    """Print a decorative separator"""
    print("\033[34m" + "â”€" * 80 + "\033[0m")

def chat_with_socrates(model, tokenizer):
    """Interactive chat loop"""
    
    # Welcome message
    welcome_box = """
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘  ğŸ“ Welcome to your personal Socratic dialogue experience!                â•‘
â•‘                                                                            â•‘
â•‘  ğŸ’­ Ask deep questions about life, virtue, knowledge, and existence       â•‘
â•‘  ğŸ¤” Challenge assumptions and explore philosophical concepts               â•‘
â•‘  ğŸ“š Engage in the ancient art of dialectical reasoning                    â•‘
â•‘                                                                            â•‘
â•‘  Type 'quit', 'exit', or 'bye' to end the conversation                    â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
"""
    print("\033[35m" + welcome_box + "\033[0m")
    
    conversation_count = 0
    
    while True:
        conversation_count += 1
        
        # User input with nice prompt
        print(f"\033[36mâ”Œâ”€ Question #{conversation_count}\033[0m")
        question = input("\033[36mâ”‚ ğŸ¤· You: \033[0m")
        
        if question.lower() in ['quit', 'exit', 'bye']:
            farewell_box = """
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘  ğŸ“ "The unexamined life is not worth living." - Socrates                 â•‘
â•‘                                                                            â•‘
â•‘  Thank you for this philosophical journey. May wisdom guide your path!    â•‘
â•‘                                                                            â•‘
â•‘  ğŸŒŸ SAMNEH AI - Where Ancient Wisdom Meets Modern Technology ğŸŒŸ          â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
"""
            print("\033[35m" + farewell_box + "\033[0m")
            break
        
        print("\033[36mâ””â”€\033[0m")
        
        # Thinking animation
        print("\033[33mğŸ§  Socrates is contemplating", end="")
        for i in range(4):
            time.sleep(0.3)
            print(".", end="", flush=True)
        print("\033[0m")
        
        # Format as instruction
        prompt = f"<s>[INST] {question} [/INST]"
        
        # Generate response
        inputs = tokenizer(prompt, return_tensors="pt")
        
        # Move inputs to same device as model
        if torch.cuda.is_available():
            inputs = {k: v.cuda() for k, v in inputs.items()}
        
        with torch.no_grad():
            outputs = model.generate(
                **inputs,
                max_new_tokens=300,
                temperature=0.7,
                do_sample=True,
                pad_token_id=tokenizer.eos_token_id,
                repetition_penalty=1.1,
                eos_token_id=tokenizer.eos_token_id
            )
        
        response = tokenizer.decode(outputs[0][len(inputs["input_ids"][0]):], skip_special_tokens=True)
        
        # Format response beautifully
        print("\033[32mâ”Œâ”€ Socratic Response\033[0m")
        
        # Word wrap the response
        words = response.split()
        lines = []
        current_line = ""
        max_width = 70
        
        for word in words:
            if len(current_line + " " + word) <= max_width:
                current_line += " " + word if current_line else word
            else:
                if current_line:
                    lines.append(current_line)
                current_line = word
        
        if current_line:
            lines.append(current_line)
        
        # Print wrapped response
        for i, line in enumerate(lines):
            if i == 0:
                print(f"\033[32mâ”‚ ğŸ“ Socrates: \033[0m{line}")
            else:
                print(f"\033[32mâ”‚            \033[0m{line}")
        
        print("\033[32mâ””â”€\033[0m")
        print()  # Empty line for spacing

def main():
    """Main function"""
    clear_screen()
    print_banner()
    
    try:
        model, tokenizer = load_socrates_model()
        chat_with_socrates(model, tokenizer)
    except KeyboardInterrupt:
        print("\n\n\033[33mâš ï¸  Conversation interrupted by user\033[0m")
        print("\033[35mğŸŒŸ Thanks for using SAMNEH AI! ğŸŒŸ\033[0m")
    except Exception as e:
        print(f"\n\033[31mâŒ Error: {e}\033[0m")
        print("\033[33mğŸ’¡ Please check your setup and try again\033[0m")

if __name__ == "__main__":
    main()