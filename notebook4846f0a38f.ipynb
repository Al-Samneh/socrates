{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0df3c584",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-05T14:11:58.304474Z",
     "iopub.status.busy": "2025-08-05T14:11:58.304158Z",
     "iopub.status.idle": "2025-08-05T14:12:02.037259Z",
     "shell.execute_reply": "2025-08-05T14:12:02.036635Z",
     "shell.execute_reply.started": "2025-08-05T14:11:58.304452Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/al-samneh/anaconda3/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== SOCRATES DIALOGUE FINE-TUNING ===\n",
      "Step 1: Loading Dataset\n",
      "Loading Socrates dialogue dataset from Hugging Face...\n",
      "Dataset shape: (3412, 3)\n",
      "Columns: ['instruction', 'input', 'output']\n",
      "\n",
      "First few rows:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>instruction</th>\n",
       "      <th>input</th>\n",
       "      <th>output</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Respond as Socrates</td>\n",
       "      <td>CRITO: Who was the person, Socrates, with whom...</td>\n",
       "      <td>SOCRATES: There were two, Crito; which of them...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Respond as Socrates</td>\n",
       "      <td>CRITO: The one whom I mean was seated second f...</td>\n",
       "      <td>SOCRATES: He whom you mean, Crito, is Euthydem...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Respond as Socrates</td>\n",
       "      <td>CRITO: Neither of them are known to me, Socrat...</td>\n",
       "      <td>SOCRATES: As to their origin, I believe that t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Respond as Socrates</td>\n",
       "      <td>CRITO: But, Socrates, are you not too old? the...</td>\n",
       "      <td>SOCRATES: Certainly not, Crito; as I will prov...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Respond as Socrates</td>\n",
       "      <td>CRITO: I see no objection, Socrates, if you li...</td>\n",
       "      <td>SOCRATES: In less than no time you shall hear;...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           instruction                                              input  \\\n",
       "0  Respond as Socrates  CRITO: Who was the person, Socrates, with whom...   \n",
       "1  Respond as Socrates  CRITO: The one whom I mean was seated second f...   \n",
       "2  Respond as Socrates  CRITO: Neither of them are known to me, Socrat...   \n",
       "3  Respond as Socrates  CRITO: But, Socrates, are you not too old? the...   \n",
       "4  Respond as Socrates  CRITO: I see no objection, Socrates, if you li...   \n",
       "\n",
       "                                              output  \n",
       "0  SOCRATES: There were two, Crito; which of them...  \n",
       "1  SOCRATES: He whom you mean, Crito, is Euthydem...  \n",
       "2  SOCRATES: As to their origin, I believe that t...  \n",
       "3  SOCRATES: Certainly not, Crito; as I will prov...  \n",
       "4  SOCRATES: In less than no time you shall hear;...  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Socrates Dialogue Fine-tuning for Kaggle\n",
    "# This notebook fine-tunes a language model on Plato's Socrates dialogues\n",
    "\n",
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import json\n",
    "import re\n",
    "from datasets import load_dataset, Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# These are the libraries we will use to fine-tune the model\n",
    "from transformers import (\n",
    "    AutoTokenizer, \n",
    "    AutoModelForCausalLM, \n",
    "    Trainer, \n",
    "    TrainingArguments\n",
    ")\n",
    "\n",
    "\n",
    "from tqdm import tqdm # for progress bar\n",
    "\n",
    "import warnings # to ignore warnings\n",
    "\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"=== SOCRATES DIALOGUE FINE-TUNING ===\")\n",
    "print(\"=== GPU SETUP ===\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA version: {torch.version.cuda}\")\n",
    "    print(f\"GPU device: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
    "    print(f\"Number of GPUs: {torch.cuda.device_count()}\")\n",
    "    # Clear GPU cache for fresh start\n",
    "    torch.cuda.empty_cache()\n",
    "    print(\"GPU cache cleared\")\n",
    "else:\n",
    "    print(\"WARNING: CUDA not available, will use CPU (training will be slow)\")\n",
    "\n",
    "print(f\"Device selected: {device}\")\n",
    "print(\"\\nStep 1: Loading Dataset\")\n",
    "print(\"Loading Socrates dialogue dataset from Hugging Face...\")\n",
    "\n",
    "# Load the dataset using the Hugging Face datasets library\n",
    "dataset = load_dataset(\"tylercross/platos_socrates_no_context\")\n",
    "df = dataset['train'].to_pandas()\n",
    "\n",
    "# Display dataset information\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "print(f\"Columns: {list(df.columns)}\")\n",
    "print(\"\\nFirst few rows:\")\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e5626e0c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-05T14:12:02.038676Z",
     "iopub.status.busy": "2025-08-05T14:12:02.038358Z",
     "iopub.status.idle": "2025-08-05T14:12:02.056588Z",
     "shell.execute_reply": "2025-08-05T14:12:02.055329Z",
     "shell.execute_reply.started": "2025-08-05T14:12:02.038658Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Step 2: Data Preprocessing ===\n",
      "Removing unnecessary columns...\n",
      "Dataset shape after removing 'instruction' column: (3412, 2)\n",
      "Remaining columns: ['input', 'output']\n",
      "\n",
      "Missing values:\n",
      "input     0\n",
      "output    0\n",
      "dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>input</th>\n",
       "      <th>output</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CRITO: Who was the person, Socrates, with whom...</td>\n",
       "      <td>SOCRATES: There were two, Crito; which of them...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CRITO: The one whom I mean was seated second f...</td>\n",
       "      <td>SOCRATES: He whom you mean, Crito, is Euthydem...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CRITO: Neither of them are known to me, Socrat...</td>\n",
       "      <td>SOCRATES: As to their origin, I believe that t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CRITO: But, Socrates, are you not too old? the...</td>\n",
       "      <td>SOCRATES: Certainly not, Crito; as I will prov...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>CRITO: I see no objection, Socrates, if you li...</td>\n",
       "      <td>SOCRATES: In less than no time you shall hear;...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               input  \\\n",
       "0  CRITO: Who was the person, Socrates, with whom...   \n",
       "1  CRITO: The one whom I mean was seated second f...   \n",
       "2  CRITO: Neither of them are known to me, Socrat...   \n",
       "3  CRITO: But, Socrates, are you not too old? the...   \n",
       "4  CRITO: I see no objection, Socrates, if you li...   \n",
       "\n",
       "                                              output  \n",
       "0  SOCRATES: There were two, Crito; which of them...  \n",
       "1  SOCRATES: He whom you mean, Crito, is Euthydem...  \n",
       "2  SOCRATES: As to their origin, I believe that t...  \n",
       "3  SOCRATES: Certainly not, Crito; as I will prov...  \n",
       "4  SOCRATES: In less than no time you shall hear;...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"\\n=== Step 2: Data Preprocessing ===\")\n",
    "print(\"Removing unnecessary columns...\")\n",
    "\n",
    "# Remove the 'instruction' column as it's not needed for our dialogue training\n",
    "df.drop('instruction', axis=1, inplace=True)\n",
    "print(f\"Dataset shape after removing 'instruction' column: {df.shape}\")\n",
    "print(\"Remaining columns:\", list(df.columns))\n",
    "\n",
    "# Check for any missing values\n",
    "print(f\"\\nMissing values:\")\n",
    "print(df.isnull().sum())\n",
    "\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f358c9ae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d034ada2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-05T14:12:02.057953Z",
     "iopub.status.busy": "2025-08-05T14:12:02.057640Z",
     "iopub.status.idle": "2025-08-05T14:12:02.209054Z",
     "shell.execute_reply": "2025-08-05T14:12:02.208261Z",
     "shell.execute_reply.started": "2025-08-05T14:12:02.057935Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Step 3: Data Formatting ===\n",
      "Converting DataFrame to training format...\n",
      "Created 3412 training examples\n",
      "\n",
      "Sample entry structure:\n",
      "{\n",
      "  \"prompt\": \"CRITO: Who was the person, Socrates, with whom you were talking yesterday at the Lyceum? There was such a crowd around you that I could not get within hearing, but I caught a sight of him over their heads, and I made out, as I thought, that he was a stranger with whom you were talking: who was he?\",\n",
      "  \"response\": \"SOCRATES: There were two, Crito; which of them do you mean?\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n=== Step 3: Data Formatting ===\")\n",
    "print(\"Converting DataFrame to training format...\")\n",
    "\n",
    "# Convert DataFrame to JSON format suitable for fine-tuning\n",
    "# Each entry will have a 'prompt' (question) and 'response' (Socrates' answer)\n",
    "\n",
    "json_data = []\n",
    "for _, row in df.iterrows():\n",
    "    json_entry = {\n",
    "        \"prompt\": row['input'],\n",
    "        \"response\": row['output']\n",
    "    }\n",
    "    json_data.append(json_entry)\n",
    "\n",
    "print(f\"Created {len(json_data)} training examples\")\n",
    "print(\"\\nSample entry structure:\")\n",
    "print(json.dumps(json_data[0], indent=2))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "528103cf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-05T14:12:02.211099Z",
     "iopub.status.busy": "2025-08-05T14:12:02.210842Z",
     "iopub.status.idle": "2025-08-05T14:12:02.271626Z",
     "shell.execute_reply": "2025-08-05T14:12:02.270873Z",
     "shell.execute_reply.started": "2025-08-05T14:12:02.211077Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample dialogue entries (first 3):\n",
      "\n",
      "--- Entry 1 ---\n",
      "Prompt: CRITO: Who was the person, Socrates, with whom you were talking yesterday at the Lyceum? There was such a crowd around you that I could not get within hearing, but I caught a sight of him over their heads, and I made out, as I thought, that he was a stranger with whom you were talking: who was he?\n",
      "Response: SOCRATES: There were two, Crito; which of them do you mean?\n",
      "\n",
      "--- Entry 2 ---\n",
      "Prompt: CRITO: The one whom I mean was seated second from you on the right-hand side. In the middle was Cleinias the young son of Axiochus, who has wonderfully grown; he is only about the age of my own Critobulus, but he is much forwarder and very good-looking: the other is thin and looks younger than he is.\n",
      "Response: SOCRATES: He whom you mean, Crito, is Euthydemus; and on my left hand there was his brother Dionysodorus, who also took part in the conversation.\n",
      "\n",
      "--- Entry 3 ---\n",
      "Prompt: CRITO: Neither of them are known to me, Socrates; they are a new importation of Sophists, as I should imagine. Of what country are they, and what is their line of wisdom?\n",
      "Response: SOCRATES: As to their origin, I believe that they are natives of this part of the world, and have migrated from Chios to Thurii; they were driven out of Thurii, and have been living for many years past in these regions. As to their wisdom, about which you ask, Crito, they are wonderful--consummate! I never knew what the true pancratiast was before; they are simply made up of fighting, not like the two Acarnanian brothers who fight with their bodies only, but this pair of heroes, besides being perfect in the use of their bodies, are invincible in every sort of warfare; for they are capital at fighting in armour, and will teach the art to any one who pays them; and also they are most skilful in legal warfare; they will plead themselves and teach others to speak and to compose speeches which will have an effect upon the courts. And this was only the beginning of their wisdom, but they have at last carried out the pancratiastic art to the very end, and have mastered the only mode of fighting which had been hitherto neglected by them; and now no one dares even to stand up against them: such is their skill in the war of words, that they can refute any proposition whether true or false. Now I am thinking, Crito, of placing myself in their hands; for they say that in a short time they can impart their skill to any one.\n"
     ]
    }
   ],
   "source": [
    "# Display a few sample entries to understand the data structure\n",
    "print(\"Sample dialogue entries (first 3):\")\n",
    "for i, entry in enumerate(json_data[:3]):\n",
    "    print(f\"\\n--- Entry {i+1} ---\")\n",
    "    print(f\"Prompt: {entry['prompt']}\")\n",
    "    print(f\"Response: {entry['response']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76c7eba8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-05T14:12:02.272857Z",
     "iopub.status.busy": "2025-08-05T14:12:02.272598Z",
     "iopub.status.idle": "2025-08-05T14:12:02.346647Z",
     "shell.execute_reply": "2025-08-05T14:12:02.345851Z",
     "shell.execute_reply.started": "2025-08-05T14:12:02.272836Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ANYTUS:', 'MENEXENUS:', 'POLUS:', 'THEODORUS:', 'LACHES:', 'EUDICUS:', 'CRITIAS:', 'ION:', 'CALLICLES:', 'CHAEREPHON:', 'CRATYLUS:', 'PROTARCHUS:', 'LYSIMACHUS:', 'DIALOGUE:', 'NICIAS:', 'SOCRATES:', 'MELESIAS:', 'BOY:', 'MENO:', 'THEAETETUS:', 'PHILEBUS:', 'EUTHYPHRO:', 'CRITO:', 'STRANGER:', 'TIMAEUS:', 'GORGIAS:', 'HIPPIAS:', 'PHAEDRUS:'}\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n=== Step 4: Data Analysis ===\")\n",
    "print(\"Analyzing speaker names in the dialogue...\")\n",
    "\n",
    "def extract_speaker_names(json_data):\n",
    "    \n",
    "    \"\"\"Extract all speaker names that occur before colons in the dialogue text\"\"\"\n",
    "    all_speakers = []\n",
    "    \n",
    "    # Convert JSON to string for regex processing\n",
    "    json_str = json.dumps(json_data, ensure_ascii=False)\n",
    "    \n",
    "    # Find all speaker names (2+ capital letters followed by colon), because all speaker names are capitalized\n",
    "    # This pattern matches names like \"SOCRATES:\", \"MENO:\", etc.\n",
    "    pattern = r'\\b[A-Z]{2,}:'\n",
    "    matches = re.findall(pattern, json_str)\n",
    "    \n",
    "    return matches\n",
    "\n",
    "# Find all speaker names in the dataset\n",
    "speaker_names = extract_speaker_names(json_data)\n",
    "unique_speakers = set(speaker_names)\n",
    "\n",
    "print(f\"Found {len(unique_speakers)} unique speakers in the dialogues:\")\n",
    "print(sorted(unique_speakers))\n",
    "print(f\"\\nTotal speaker name occurrences: {len(speaker_names)}\")\n",
    "\n",
    "# Count frequency of each speaker\n",
    "from collections import Counter\n",
    "speaker_counts = Counter(speaker_names)\n",
    "print(f\"\\nMost common speakers:\")\n",
    "for speaker, count in speaker_counts.most_common(5):\n",
    "    print(f\"  {speaker} {count} times\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28ee1582",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-05T14:12:45.650943Z",
     "iopub.status.busy": "2025-08-05T14:12:45.650482Z",
     "iopub.status.idle": "2025-08-05T14:12:45.692696Z",
     "shell.execute_reply": "2025-08-05T14:12:45.692169Z",
     "shell.execute_reply.started": "2025-08-05T14:12:45.650921Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned 3412 entries\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n=== Step 5: Data Cleaning ===\")\n",
    "print(\"Removing speaker names from dialogue text...\")\n",
    "\n",
    "# Clean the data by removing speaker names and extra whitespace\n",
    "# This makes the text more natural for training the language model\n",
    "cleaned_json_data = []\n",
    "\n",
    "for entry in json_data:\n",
    "    cleaned_entry = {\n",
    "        # Remove speaker names (e.g., \"SOCRATES: \" becomes \"\")\n",
    "        \"prompt\": re.sub(r'\\b[A-Z]{2,}:\\s*', '', entry['prompt']).strip(),\n",
    "        \"response\": re.sub(r'\\b[A-Z]{2,}:\\s*', '', entry['response']).strip()\n",
    "    }\n",
    "    # Only keep entries that have content after cleaning\n",
    "    if cleaned_entry['prompt'] and cleaned_entry['response']:\n",
    "        cleaned_json_data.append(cleaned_entry)\n",
    "\n",
    "print(f\"Cleaned {len(cleaned_json_data)} entries\")\n",
    "print(f\"Removed {len(json_data) - len(cleaned_json_data)} empty entries\")\n",
    "\n",
    "# Show before/after comparison\n",
    "print(\"\\n--- Before Cleaning ---\")\n",
    "print(f\"Original: {json_data[0]['prompt'][:100]}...\")\n",
    "print(\"\\n--- After Cleaning ---\")\n",
    "print(f\"Cleaned: {cleaned_json_data[0]['prompt'][:100]}...\")\n",
    "\n",
    "# Update json_data to the cleaned version\n",
    "json_data = cleaned_json_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e8699a4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-05T14:12:47.521641Z",
     "iopub.status.busy": "2025-08-05T14:12:47.521390Z",
     "iopub.status.idle": "2025-08-05T14:12:47.566153Z",
     "shell.execute_reply": "2025-08-05T14:12:47.565463Z",
     "shell.execute_reply.started": "2025-08-05T14:12:47.521624Z"
    },
    "trusted": true
   },
   "source": [
    "# Display sample cleaned entries to verify the cleaning worked\n",
    "print(\"=== Sample Cleaned Entries ===\")\n",
    "for i, entry in enumerate(json_data[:2]):\n",
    "    print(f\"\\n--- Sample {i+1} ---\")\n",
    "    print(f\"Prompt: {entry['prompt']}\")\n",
    "    print(f\"Response: {entry['response']}\")\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5afdec66",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-05T14:13:00.537221Z",
     "iopub.status.busy": "2025-08-05T14:13:00.536862Z",
     "iopub.status.idle": "2025-08-05T14:15:21.457686Z",
     "shell.execute_reply": "2025-08-05T14:15:21.456196Z",
     "shell.execute_reply.started": "2025-08-05T14:13:00.537200Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "564fb66e6e184f0894216540742f342d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/287 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a787a5c4050b4128bdb7b84e7bb341e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0485dd6defbb427fa76eece6b56c3df6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/281 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d40a5619fc9b4f49841e422db931f35c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-05 14:13:15.150991: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1754403195.337468      36 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1754403195.397005      36 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f6974b4c05d4841bf4758bac8fdb351",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c57e6e83eab745ecb0e5c1e6e18ae679",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 2 files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "35865e5c0983489081dc34bd5b8c8d03",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00002.safetensors:   0%|          | 0.00/9.95G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c28878bafbf4528af6147a7977fcb90",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00002.safetensors:   0%|          | 0.00/4.48G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b814a3144d5431c9937f5ea3466c309",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "86acd57f05a54a619cc11b023713ca42",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/117 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"\\n=== Step 6: Train/Validation Split ===\")\n",
    "print(\"Splitting data into training and validation sets...\")\n",
    "\n",
    "# Split data into training and validation sets (90% train, 10% validation)\n",
    "\n",
    "train_data, val_data = train_test_split(\n",
    "    json_data, \n",
    "    test_size=0.1, \n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "print(f\"Training set size: {len(train_data)}\")\n",
    "print(f\"Validation set size: {len(val_data)}\")\n",
    "\n",
    "print(\"\\n=== Step 7: Model Setup ===\")\n",
    "print(\"Initializing pre-trained model and tokenizer...\")\n",
    "\n",
    "# Model configuration - you can change this to other models if needed\n",
    "MODEL_NAME = \"tiiuae/falcon-7b\"  \n",
    "print(f\"Loading model: {MODEL_NAME}\")\n",
    "\n",
    "# Load tokenizer first\n",
    "print(\"Loading tokenizer...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "# Add padding token if it doesn't exist\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    print(\"Added padding token\")\n",
    "\n",
    "# Load model with GPU optimizations for training\n",
    "print(\"Loading model...\")\n",
    "print(f\"Target device: {device}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    # Clear GPU cache before loading model\n",
    "    torch.cuda.empty_cache()\n",
    "    print(\"GPU cache cleared before model loading\")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    torch_dtype=torch.float16,  # Use half precision to save memory\n",
    "    device_map=\"auto\" if torch.cuda.is_available() else None,  # Auto GPU placement if available\n",
    "    trust_remote_code=True,  # Required for some models\n",
    "    low_cpu_mem_usage=True,  # Reduce CPU memory usage during loading\n",
    ")\n",
    "\n",
    "print(\"Model loaded successfully!\")\n",
    "print(f\"Model parameters: {model.num_parameters():,}\")\n",
    "print(f\"Model device: {next(model.parameters()).device}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU memory allocated: {torch.cuda.memory_allocated() / 1e9:.2f} GB\")\n",
    "    print(f\"GPU memory cached: {torch.cuda.memory_reserved() / 1e9:.2f} GB\")\n",
    "    \n",
    "    # Ensure model is on GPU if available\n",
    "    model_device = next(model.parameters()).device\n",
    "    if model_device.type != 'cuda':\n",
    "        print(\"Moving model to GPU...\")\n",
    "        model = model.to(device)\n",
    "        print(f\"Model moved to: {next(model.parameters()).device}\")\n",
    "    else:\n",
    "        print(\"Model already on GPU âœ“\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "698af3bf-d77a-403f-a920-c7d2d510c351",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-05T14:19:20.405894Z",
     "iopub.status.busy": "2025-08-05T14:19:20.405631Z",
     "iopub.status.idle": "2025-08-05T14:19:20.434429Z",
     "shell.execute_reply": "2025-08-05T14:19:20.433513Z",
     "shell.execute_reply.started": "2025-08-05T14:19:20.405875Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "list indices must be integers or slices, not str",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_36/4104892711.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtokenized_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenize_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjson_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_36/2714047609.py\u001b[0m in \u001b[0;36mtokenize_function\u001b[0;34m(examples)\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mtokenize_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexamples\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexamples\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"prompt\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"max_length\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtruncation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: list indices must be integers or slices, not str"
     ]
    }
   ],
   "source": [
    "print(\"\\n=== Step 8: Data Tokenization ===\")\n",
    "print(\"Converting data to HuggingFace Dataset format and tokenizing...\")\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    \"\"\"Tokenize the text for causal language modeling with GPU optimization\"\"\"\n",
    "    # Combine prompt and response for causal language modeling\n",
    "    # Keep your format: just text input -> text output (no speaker names)\n",
    "    texts = [f\"{prompt}\\n{response}\" for prompt, response in zip(examples[\"prompt\"], examples[\"response\"])]\n",
    "    \n",
    "    tokenized = tokenizer(\n",
    "        texts,\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=1024,  # Increased for longer philosophical discussions\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    \n",
    "    # For causal LM, labels are the same as input_ids\n",
    "    tokenized[\"labels\"] = tokenized[\"input_ids\"].clone()\n",
    "    \n",
    "    # Note: Datasets will handle moving to GPU during training automatically\n",
    "    # with DataLoader, so we don't need to explicitly move here\n",
    "    return tokenized\n",
    "\n",
    "# Convert training and validation data to HuggingFace Dataset format\n",
    "print(\"Creating Dataset objects...\")\n",
    "train_dataset = Dataset.from_list(train_data)\n",
    "val_dataset = Dataset.from_list(val_data)\n",
    "\n",
    "print(f\"Training dataset created with {len(train_dataset)} examples\")\n",
    "print(f\"Validation dataset created with {len(val_dataset)} examples\")\n",
    "print(f\"Dataset features: {train_dataset.features}\")\n",
    "\n",
    "# Tokenize both datasets\n",
    "print(\"Tokenizing training dataset...\")\n",
    "tokenized_train_dataset = train_dataset.map(\n",
    "    tokenize_function, \n",
    "    batched=True,\n",
    "    remove_columns=[\"prompt\", \"response\"],  # Remove original columns\n",
    "    desc=\"Tokenizing training data\"\n",
    ")\n",
    "\n",
    "print(\"Tokenizing validation dataset...\")\n",
    "tokenized_val_dataset = val_dataset.map(\n",
    "    tokenize_function, \n",
    "    batched=True,\n",
    "    remove_columns=[\"prompt\", \"response\"],  # Remove original columns\n",
    "    desc=\"Tokenizing validation data\"\n",
    ")\n",
    "\n",
    "print(f\"Tokenized training dataset size: {len(tokenized_train_dataset)}\")\n",
    "print(f\"Tokenized validation dataset size: {len(tokenized_val_dataset)}\")\n",
    "print(f\"Tokenized features: {tokenized_train_dataset.features}\")\n",
    "\n",
    "# Display sample tokenized data\n",
    "print(\"\\nSample tokenized entry from training set:\")\n",
    "sample = tokenized_train_dataset[0]\n",
    "print(f\"Input IDs shape: {len(sample['input_ids'])}\")\n",
    "print(f\"First 10 tokens: {sample['input_ids'][:10]}\")\n",
    "print(f\"Decoded sample: {tokenizer.decode(sample['input_ids'][:50], skip_special_tokens=True)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad54b19b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7b82edb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7134f75",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f65e3b56",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06bd962a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c251b6c4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-05T14:18:19.238314Z",
     "iopub.status.busy": "2025-08-05T14:18:19.237810Z",
     "iopub.status.idle": "2025-08-05T14:18:19.338901Z",
     "shell.execute_reply": "2025-08-05T14:18:19.337986Z",
     "shell.execute_reply.started": "2025-08-05T14:18:19.238290Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tokenized_dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_36/3465998164.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtraining_args\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m     \u001b[0mtrain_dataset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtokenized_dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m )\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tokenized_dataset' is not defined"
     ]
    }
   ],
   "source": [
    "print(\"\\n=== Step 9: Training Configuration ===\")\n",
    "print(\"Setting up training arguments and trainer...\")\n",
    "\n",
    "# Check GPU memory before training\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU memory before training setup: {torch.cuda.memory_allocated() / 1e9:.2f} GB allocated\")\n",
    "    print(f\"GPU memory available: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB total\")\n",
    "\n",
    "# Training configuration optimized for GPU with validation\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./socrates-finetuned-model\",  # Output directory\n",
    "    per_device_train_batch_size=2,  # Batch size per device (adjust based on GPU memory)\n",
    "    per_device_eval_batch_size=2,   # Evaluation batch size\n",
    "    gradient_accumulation_steps=4,  # Effective batch size = 2 * 4 = 8\n",
    "    num_train_epochs=3,  # Number of training epochs\n",
    "    logging_dir=\"./logs\",  # Logging directory\n",
    "    save_total_limit=2,  # Keep only the last 2 checkpoints to save space\n",
    "    logging_steps=50,  # Log every 50 steps\n",
    "    save_steps=250,  # Save checkpoint every 250 steps\n",
    "    eval_steps=100,  # Evaluate every 100 steps\n",
    "    warmup_steps=100,  # Number of warmup steps for learning rate scheduler\n",
    "    learning_rate=2e-5,  # Learning rate (reduced for fine-tuning)\n",
    "    weight_decay=0.01,  # Weight decay for regularization\n",
    "    fp16=True if torch.cuda.is_available() else False,  # Use mixed precision only on GPU\n",
    "    dataloader_pin_memory=True if torch.cuda.is_available() else False,  # Enable pin memory for GPU\n",
    "    dataloader_num_workers=0,  # Reduce workers to save memory\n",
    "    evaluation_strategy=\"steps\",  # Enable evaluation during training\n",
    "    save_strategy=\"steps\",  # Save based on steps\n",
    "    load_best_model_at_end=True,  # Load the best model at the end\n",
    "    metric_for_best_model=\"eval_loss\",  # Use evaluation loss as metric\n",
    "    greater_is_better=False,  # Lower loss is better\n",
    "    remove_unused_columns=False,  # Keep all columns\n",
    "    report_to=\"none\",  # Disable wandb/tensorboard logging\n",
    "    push_to_hub=False,  # Don't push to model hub\n",
    "    optim=\"adamw_torch\",  # Use PyTorch AdamW optimizer\n",
    "    gradient_checkpointing=True,  # Save memory by trading compute for memory\n",
    ")\n",
    "\n",
    "print(\"Training arguments configured successfully!\")\n",
    "print(f\"Effective batch size: {training_args.per_device_train_batch_size * training_args.gradient_accumulation_steps}\")\n",
    "print(f\"Total training steps: {len(tokenized_train_dataset) // (training_args.per_device_train_batch_size * training_args.gradient_accumulation_steps) * training_args.num_train_epochs}\")\n",
    "\n",
    "# Initialize trainer with both training and validation datasets\n",
    "print(\"Initializing trainer...\")\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train_dataset,  # Use training dataset\n",
    "    eval_dataset=tokenized_val_dataset,     # Use validation dataset\n",
    "    tokenizer=tokenizer,  # Add tokenizer for proper saving\n",
    ")\n",
    "\n",
    "print(\"Trainer initialized successfully!\")\n",
    "print(f\"Training dataset size: {len(tokenized_train_dataset)}\")\n",
    "print(f\"Validation dataset size: {len(tokenized_val_dataset)}\")\n",
    "\n",
    "# Start training\n",
    "print(\"\\n=== Step 10: Model Training ===\")\n",
    "print(\"Starting model training with validation monitoring...\")\n",
    "print(\"This may take a while depending on your hardware.\")\n",
    "print(f\"Training on {len(tokenized_train_dataset)} examples for {training_args.num_train_epochs} epochs\")\n",
    "print(f\"Validating on {len(tokenized_val_dataset)} examples\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU memory before training: {torch.cuda.memory_allocated() / 1e9:.2f} GB\")\n",
    "    print(\"Training will use GPU acceleration âœ“\")\n",
    "else:\n",
    "    print(\"Training will use CPU (this will be slower)\")\n",
    "\n",
    "# Clear cache before training\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "# Train the model\n",
    "trainer.train()\n",
    "\n",
    "print(\"Training completed successfully!\")\n",
    "print(\"Best model has been loaded based on validation loss.\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU memory after training: {torch.cuda.memory_allocated() / 1e9:.2f} GB\")\n",
    "    # Clear cache after training\n",
    "    torch.cuda.empty_cache()\n",
    "    print(\"GPU cache cleared after training\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4869c2bd",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-08-05T14:12:02.516881Z",
     "iopub.status.idle": "2025-08-05T14:12:02.517166Z",
     "shell.execute_reply": "2025-08-05T14:12:02.517024Z",
     "shell.execute_reply.started": "2025-08-05T14:12:02.517015Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "print(\"\\n=== Step 11: Save Fine-tuned Model ===\")\n",
    "print(\"Saving the fine-tuned model and tokenizer...\")\n",
    "\n",
    "# Save the fine-tuned model and tokenizer\n",
    "model_save_path = \"socrates-finetuned-model\"\n",
    "trainer.save_model(model_save_path)\n",
    "tokenizer.save_pretrained(model_save_path)\n",
    "\n",
    "print(f\"Model saved successfully to: {model_save_path}\")\n",
    "print(\"The model can now be loaded for inference or further training.\")\n",
    "print(\"Note: The best model (based on validation loss) has been saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f5b5a11",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-08-05T14:12:02.518325Z",
     "iopub.status.idle": "2025-08-05T14:12:02.518614Z",
     "shell.execute_reply": "2025-08-05T14:12:02.518453Z",
     "shell.execute_reply.started": "2025-08-05T14:12:02.518441Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "print(\"\\n=== Step 12: Test the Fine-tuned Model ===\")\n",
    "print(\"Testing the model with philosophical questions...\")\n",
    "\n",
    "def generate_socrates_response(prompt, max_new_tokens=100):\n",
    "    \"\"\"Generate a response using the fine-tuned Socrates model with GPU optimization\"\"\"\n",
    "    # Format the input to match training format\n",
    "    formatted_input = f\"{prompt}\\n\"\n",
    "    \n",
    "    # Tokenize the input\n",
    "    input_ids = tokenizer.encode(formatted_input, return_tensors=\"pt\")\n",
    "    \n",
    "    # Move to same device as model (GPU if available)\n",
    "    model_device = next(model.parameters()).device\n",
    "    input_ids = input_ids.to(model_device)\n",
    "    \n",
    "    # Generate response with GPU acceleration\n",
    "    with torch.no_grad():\n",
    "        if torch.cuda.is_available():\n",
    "            # Use GPU-optimized generation\n",
    "            with torch.cuda.amp.autocast():  # Use automatic mixed precision\n",
    "                output = model.generate(\n",
    "                    input_ids,\n",
    "                    max_new_tokens=max_new_tokens,\n",
    "                    temperature=0.7,  # Add some randomness\n",
    "                    do_sample=True,   # Use sampling instead of greedy\n",
    "                    pad_token_id=tokenizer.eos_token_id,\n",
    "                    no_repeat_ngram_size=2,  # Avoid repetition\n",
    "                    eos_token_id=tokenizer.eos_token_id,\n",
    "                    use_cache=True  # Enable KV cache for faster generation\n",
    "                )\n",
    "        else:\n",
    "            # CPU generation (fallback)\n",
    "            output = model.generate(\n",
    "                input_ids,\n",
    "                max_new_tokens=max_new_tokens,\n",
    "                temperature=0.7,\n",
    "                do_sample=True,\n",
    "                pad_token_id=tokenizer.eos_token_id,\n",
    "                no_repeat_ngram_size=2,\n",
    "                eos_token_id=tokenizer.eos_token_id\n",
    "            )\n",
    "    \n",
    "    # Decode and return the response\n",
    "    full_response = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    \n",
    "    # Remove the input prompt from the response and clean up\n",
    "    response = full_response[len(formatted_input):].strip()\n",
    "    \n",
    "    # Clean up any remaining artifacts\n",
    "    if response.startswith('\\n'):\n",
    "        response = response[1:].strip()\n",
    "    \n",
    "    return response\n",
    "\n",
    "# Test the model with philosophical questions\n",
    "test_prompts = [\n",
    "    \"What is the meaning of life?\",\n",
    "    \"How should one live a good life?\", \n",
    "    \"What is wisdom?\",\n",
    "    \"Tell me about virtue.\",\n",
    "    \"What is justice?\"\n",
    "]\n",
    "\n",
    "print(\"Testing the fine-tuned Socrates model:\")\n",
    "print(\"Format: Your text input â†’ Model's text output (no speaker names)\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"Running inference on GPU: {torch.cuda.get_device_name(0)} âœ“\")\n",
    "else:\n",
    "    print(\"Running inference on CPU\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "for i, prompt in enumerate(test_prompts, 1):\n",
    "    print(f\"\\n{i}. Input: {prompt}\")\n",
    "    try:\n",
    "        response = generate_socrates_response(prompt, max_new_tokens=75)\n",
    "        print(f\"   Output: {response}\")\n",
    "    except Exception as e:\n",
    "        print(f\"   Error generating response: {e}\")\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "print(\"\\nModel testing completed!\")\n",
    "print(\"Your Socrates dialogue model is ready for use!\")\n",
    "print(\"The model takes text input and returns text output, following the Socratic dialogue style.\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"\\n=== Final GPU Status ===\")\n",
    "    print(f\"GPU memory in use: {torch.cuda.memory_allocated() / 1e9:.2f} GB\")\n",
    "    print(f\"GPU memory cached: {torch.cuda.memory_reserved() / 1e9:.2f} GB\")\n",
    "    print(\"All operations completed successfully on GPU! ðŸš€\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [],
   "dockerImageVersionId": 31090,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
