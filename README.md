# Socrates AI - A Journey from Simple Q&A to Advanced Fine-Tuning

> "The unexamined life is not worth living" - Socrates

This repository chronicles my learning journey in building a comprehensive Socratic dialogue generation and fine-tuning system, evolving from basic API calls to sophisticated multi-stage model training.

## 🌟 Featured Examples

See hand-picked questions and Socratic responses generated by the model:

- Full examples: `examples/sample_dialogues.md`

Quick preview:

```text
🤷 You: If no one can truly know what is good without wisdom, can any person act justly without first seeking to know themselves?

🎓 Socrates: You ask a difficult question, my friend... (see examples for full response)
```

### Origin Story: From a HF dataset to a custom training pipeline
- Started by searching Hugging Face for a Socrates dataset and testing it to reenact Socratic dialogue (e.g., Plato-derived student dialogues).
- Discovered the base LLM barely captured Socrates’ essence and style; quantization helped latency but not quality.
- Rented an H100 and upgraded the base model to Mistral-7B.
- Built scripts for synthetic data generation and created two new datasets:
  - Socratic-method Q&A pairs (character statement → Socrates response)
  - Fictional dialogues between Socrates and historical figures (e.g., Alexander the Great, Napoleon, Plato)
- Fine-tuned the model on these datasets using LORA, then used a third model as an LLM judge (Gemini) to rate responses 1–5 across criteria and retrained via DPO.
- Iterated on learning rate, dropout, and other training settings to further improve accuracy.
- Although the final model benefits from stronger hardware, external evaluation by another AI judged the style/prose to convincingly reflect Socrates’ voice.

## 📖 My Learning Journey

### Phase 1: The Beginning - Simple Q&A Generation
**Goal:** Create basic philosophical question-answer pairs using OpenAI's API

**What I Built:**
- Simple script to generate Socratic Q&A pairs
- Basic JSON storage format with `input`/`output` structure
- Single-file approach with hardcoded prompts

**Challenges Faced:**
- API rate limiting and timeout issues
- Lost progress when scripts crashed (no incremental saving)
- Poor error handling leading to difficult debugging

### Phase 2: Scaling Up - Conversational Dialogues
**Goal:** Move beyond Q&A to full conversations between Socrates and historical figures

**What I Learned:**
- Importance of modular code architecture
- Need for robust error handling and logging
- Value of incremental progress saving

**Key Evolution:**
- Created 26+ authentic historical character profiles
- Implemented dialogue parsing to convert full conversations into training pairs
- Added automatic backup systems and progress tracking

### Phase 3: Production Ready - Robust Data Generation
**Goal:** Create production-quality data generation pipeline

**Major Breakthroughs:**
- **Modular Architecture:** Separated concerns into dedicated modules
- **Error Recovery:** Implemented incremental saving and resume functionality
- **Data Quality:** Added validation and cleaning pipelines
- **Scalability:** Built for generating thousands of dialogue pairs

### Phase 4: Advanced Training - SFT & DPO Pipeline
**Goal:** Train my own Socratic AI using the generated data

**Technical Deep Dive:**
- **Supervised Fine-Tuning (SFT):** Train Mistral-7B on dialogue data
- **Direct Preference Optimization (DPO):** Improve responses using Gemini as judge
- **Multi-environment Testing:** Debugged across Kaggle, VAST AI, and local setups

## 🏗️ Final Project Architecture

```
socrates/
├── src/socrates_ai/           # Core modules (modularized in Phase 2)
│   ├── characters.py          # 26+ historical character profiles
│   ├── dialogue_generator.py  # Robust dialogue generation
│   └── question_generator.py  # Q&A pair generation
├── scripts/                   # Production-ready scripts
│   ├── generate_dialogues.py  # Multi-character dialogue generation
│   ├── generate_questions.py  # Philosophical Q&A generation
│   ├── sft_finetune.py       # Stage 1: Supervised Fine-Tuning
│   ├── dpo_improvement.py    # Stage 2: DPO with Gemini judge
│   └── web_app.py           # Interactive chat interface
├── data/                     # Generated datasets with backups
├── logs/                     # Comprehensive logging system
└── config/                   # Configuration and topics
```

## 🛠️ Quick Start (Battle-Tested Setup)

### 1. Environment Setup
```bash
# Install packages (debugged across multiple environments)
pip install -r requirements.txt

# CRITICAL: Install bitsandbytes from source for H100 compatibility
git clone https://github.com/bitsandbytes-foundation/bitsandbytes.git
cd bitsandbytes/
cmake -DCOMPUTE_BACKEND=cuda -S .
make
pip install .
cd ..
```

### 2. Data Generation Pipeline
```bash
# Generate philosophical Q&A pairs
python scripts/generate_questions.py

# Generate historical character dialogues
python scripts/generate_dialogues.py
```

### 3. Model Training Pipeline
```bash
# Stage 1: Supervised Fine-Tuning (requires HuggingFace token)
python scripts/sft_finetune.py

# Stage 2: DPO Improvement (requires Gemini API key)
python scripts/dpo_improvement.py

# Deploy: Interactive chat interface
python scripts/web_app.py
```

## 🏆 Technical Achievements

### Data Generation Mastery
- **26+ Historical Characters:** Each with authentic personality profiles and speaking styles
- **HELLA Dialogue Pairs:** Generated through robust, crash-resistant pipeline
- **Multi-format Output:** Both conversational exchanges and Q&A pairs
- **Quality Assurance:** Automated cleaning and validation systems

### Advanced Training Pipeline
- **SFT (Supervised Fine-Tuning):** Mistral-7B trained on philosophical dialogues
- **DPO (Direct Preference Optimization):** 500-prompt evaluation using Gemini-1.5-Flash as judge
- **Memory Optimization:** 4-bit quantization, gradient checkpointing for lower performing GPUs
- **Multi-environment Compatibility:** Kaggle, VAST AI, local development

### Production Engineering
- **Robust Error Handling:** Graceful recovery from API failures and memory errors
- **Incremental Processing:** Never lose progress, resume from any point
- **Comprehensive Logging:** Track every API call and processing step
- **Automatic Backups:** Multiple backup strategies prevent data loss

## 📊 Dataset Formats

### Conversational Dialogues
```json
{
  "dataset": [
    {
      "input": "Socrates, I believe virtue can be taught like any other skill.",
      "output": "Ah, my dear Aristotle, but if virtue is teachable, why do we see children of virtuous parents who lack virtue themselves?"
    }
  ]
}
```

### Q&A Training Pairs
```json
{
  "dataset": [
    {
      "input": "What is the nature of justice?", 
      "output": "Justice, my friend, appears to be giving each their due. But what constitutes 'due'? Is it what the law prescribes, what benefits society, or something else entirely?"
    }
  ]
}
```

## 🎭 Historical Character Library

**26+ Authentic Personalities:**
- **Ancient Philosophers:** Aristotle, Plato, Confucius, Lao Tzu
- **Modern Thinkers:** Kant, Nietzsche, Descartes, Spinoza
- **Leaders:** Alexander the Great, Julius Caesar, Napoleon THE GREAT, Churchill
- **Scientists:** Einstein, Newton, Tesla, Marie Curie, Darwin
- **Writers:** Shakespeare, Tolkien, Hemingway

Each character includes:
- Historical context and biography
- Authentic speaking patterns and vocabulary
- Key philosophical positions and beliefs
- Personality traits and quirks

## 🔧 Key Features Developed

### Reliability Engineering
- **Crash Recovery:** Resume from any interruption point
- **Progress Tracking:** Real-time status updates and completion estimates
- **Error Handling:** Graceful degradation and informative error messages
- **Data Validation:** Automatic quality checks and cleaning

### Scalability Solutions
- **Batch Processing:** Efficient handling of large topic lists
- **Memory Management:** Optimized for limited GPU memory environments
- **Rate Limiting:** Respectful API usage with automatic retry logic
- **Parallel Processing:** Multi-GPU training support


## 📈 Performance Metrics

- **Data Generation:** 1000+ dialogue pairs/hour
- **Training Time:** ~6-12 hours for full SFT+DPO pipeline
- **Memory Usage:** Optimized for 16GB GPU (T4/V100)
- **Quality Score:** 85%+ preference by Gemini judge evaluation


## 🚀 Getting Started Commands

```bash
# 1. Clone and setup
git clone <your-repo>
cd socrates
pip install -r requirements.txt

# 2. Install bitsandbytes from source (H100 requirement)
git clone https://github.com/bitsandbytes-foundation/bitsandbytes.git
cd bitsandbytes && cmake -DCOMPUTE_BACKEND=cuda -S . && make && pip install . && cd ..

# 3. Generate training data
python scripts/generate_questions.py    # Q&A pairs
python scripts/generate_dialogues.py   # Character conversations  

# 4. Train your Socrates AI
python scripts/sft_finetune.py         # Stage 1: SFT
python scripts/dpo_improvement.py      # Stage 2: DPO

# 5. Deploy and test
python scripts/web_app.py              # Web interface
```

## 🎉 What's Next?

This project demonstrates the complete journey from idea to production AI system. The techniques learned here apply to:

- **Custom AI assistants** for specific domains
- **Character-based chatbots** with consistent personalities  
- **Educational AI tutors** using Socratic methodology
- **Content generation** for creative writing
- **Model fine-tuning** for specialized tasks

The debugging skills and production engineering practices developed here are invaluable for any serious AI development project.

---

> *"I know that I know nothing"* - Socrates  